**Title: Integrating Apache Pinot with Python: A Comprehensive Tutorial**

**Objective:** Learn how to use Apache Pinot for real-time data analytics with Python.

### **What is Apache Pinot?**

**Apache Pinot** is a real-time distributed analytics data store that enables ultra-fast, scalable, and efficient OLAP (Online Analytical Processing) on large volumes of data. Originally developed at LinkedIn, it powers real-time analytics for various applications. Pinot is especially useful when data freshness and low-latency querying are crucial.

**Key Features of Apache Pinot**:
1. **Real-time Analytics**: It supports both real-time and batch data ingestion.
2. **Low Latency**: Provides millisecond-level response time, even with large datasets.
3. **Scalability**: Can scale horizontally, supporting complex aggregation and filtering operations.
4. **Columnar Data Store**: Optimized for efficient storage and retrieval of columnar data, which benefits OLAP queries.

### **Architecture Overview**

Apache Pinot's architecture consists of several key components:
1. **Controller**: Coordinates the cluster and manages table configurations.
2. **Broker**: Routes queries to the appropriate server to return results to the client.
3. **Server**: Stores data segments and serves queries.
4. **Minion**: Handles background tasks like data compaction. (optional)
5. **Real-time and Offline Ingestion**: Supports data ingestion from streaming sources (e.g., Apache Kafka) and batch sources (e.g., Hadoop, S3).

### **Python and Apache Pinot Integration**

Python is widely used in the data analytics ecosystem, and by integrating Python with Apache Pinot, you can create robust data analysis pipelines to work with real-time data.

We will use the **Pinot REST API** to communicate between Python and Apache Pinot. Python's `requests` library will allow us to send HTTP requests to query and interact with Pinot.

### **Prerequisites**

### **Step 1: Setting Up Apache Pinot**

```bash
   docker-compose up -d
```

First, make sure you have Apache Pinot running. You can either run it locally using Docker or set up a Pinot cluster. Once Pinot is running, you can access the web UI at `http://localhost:9000`.

### **Step 2: Ingest Sample Data into Apache Pinot**

![image](https://github.com/user-attachments/assets/814a7df8-0ebf-4874-b6b6-2a6df8a24c59)

You need to ingest data into Apache Pinot to query it. Here, we use a sample dataset containing movie ratings.

![image](https://github.com/user-attachments/assets/6e1f92bd-0eaf-4999-a940-078169de3acc)

1. **Create a Schema**:
   ```json
   {
      "schemaName": "moviesSchema",
      "dimensionFieldSpecs": [
         {
            "name": "movieId",
            "dataType": "INT"
         },
         {
            "name": "title",
            "dataType": "STRING"
          },
         {
            "name": "genres",
            "dataType": "STRING"
          }
        ],
       "metricFieldSpecs": [
         {
            "name": "rating",
            "dataType": "FLOAT"
         }
      ],
         "dateTimeFieldSpecs": [
         {
            "name": "timestamp",
            "dataType": "LONG",
            "format": "1:MINUTES:EPOCH",
            "granularity": "1:MINUTES"
         }
      ]
   }
   ```
You can use the Pinot UI or REST API to create this schema.

2. **Create a Table**:
   Define a real-time table that specifies Kafka as the ingestion source. (Video to config Pinot and Kafka: https://drive.google.com/file/d/119KfmpyWeD_1TqYoJpiXmXjRuEel82zZ/view)

```json
{
  "REALTIME": {
    "tableName": "moviesSchema",
    "tableType": "REALTIME",
    "segmentsConfig": {
      "schemaName": "moviesSchema",
      "timeColumnName": "timestamp",
      "replication": "1",
      "replicasPerPartition": "1",
      "minimizeDataMovement": false
    },
    "tenants": {
      "broker": "DefaultTenant",
      "server": "DefaultTenant",
      "tagOverrideConfig": {}
    },
    "tableIndexConfig": {
      "invertedIndexColumns": [],
      "rangeIndexColumns": [],
      "rangeIndexVersion": 2,
      "autoGeneratedInvertedIndex": false,
      "createInvertedIndexDuringSegmentGeneration": false,
      "sortedColumn": [],
      "bloomFilterColumns": [],
      "loadMode": "MMAP",
      "streamConfigs": {
        "streamType": "kafka",
        "stream.kafka.topic.name": "movies-topic",
        "stream.kafka.broker.list": "kafka:9093",
        "stream.kafka.consumer.type": "lowlevel",
        "stream.kafka.consumer.prop.auto.offset.reset": "smallest",
        "stream.kafka.consumer.factory.class.name": "org.apache.pinot.plugin.stream.kafka20.KafkaConsumerFactory",
        "stream.kafka.decoder.class.name": "org.apache.pinot.plugin.stream.kafka.KafkaJSONMessageDecoder",
        "realtime.segment.flush.threshold.rows": "0",
        "realtime.segment.flush.threshold.time": "24h",
        "realtime.segment.flush.threshold.segment.size": "100M"
      },
      "noDictionaryColumns": [],
      "onHeapDictionaryColumns": [],
      "varLengthDictionaryColumns": [],
      "enableDefaultStarTree": false,
      "enableDynamicStarTreeCreation": false,
      "aggregateMetrics": false,
      "nullHandlingEnabled": false,
      "optimizeDictionary": false,
      "optimizeDictionaryForMetrics": false,
      "noDictionarySizeRatioThreshold": 0
    },
    "metadata": {},
    "quota": {},
    "routing": {},
    "query": {},
    "ingestionConfig": {
      "continueOnError": false,
      "rowTimeValueCheck": false,
      "segmentTimeValueCheck": true
    },
    "isDimTable": false
  }
}
```
3. **Insert Data into the Table**:
   Data can be inserted into Pinot using either real-time ingestion (e.g., from Kafka) or batch ingestion (e.g., from CSV files).

   - **Real-Time Ingestion Using Kafka**:
     
     ```python
      # Producer
      from confluent_kafka import Producer
      from bson import json_util
      import json
     
      p = Producer({'bootstrap.servers':'localhost:9092'})
      data = {
          "movieId": 1,
          "title": "Inception",
          "genres": "Action, Sci-Fi",
          "rating": 4.8
      }
      
      p.produce('movies-topic', key="", value=json.dumps(data, default=json_util.default).encode('utf-8'))     
      p.flush()
      print("flush ",data)
     ```
     - Pinot will automatically consume data from Kafka and insert it into the table if properly configured.

   - **Batch Ingestion Using Files**:
     - Prepare a CSV file (e.g., `movies.csv`) with the data you want to ingest.
     - Use Pinot's `Add Table` command to ingest the batch data:

     - You can also use Python to upload data:
     
     ```python
     import requests

     upload_url = "http://localhost:9000/tables/batch"
     files = {
         'file': ('movies.csv', open('movies.csv', 'rb')),
     }
     response = requests.post(upload_url, files=files)
     print(response.status_code, response.text)
     ```

### **Step 3: Querying Pinot with Python**

Now that you have data in Pinot, you can use Python to interact with it. 
![image](https://github.com/user-attachments/assets/efc6a096-ce86-47c2-9dff-804fef830e3f)

or use the Pinot REST API to send queries.

```python
import requests
import pandas as pd

# Define the Pinot broker URL
PINOT_BROKER_URL = "http://localhost:8099/query/sql"

# Define a sample query to get movie ratings
query = {
    "sql":"select * from moviesSchema limit 10"
}

#query = {
#    "sql": "SELECT movieId, title, AVG(rating) as avgRating FROM moviesSchema GROUP BY movieId, title LIMIT 10"
#}

# Send the query to Pinot
response = requests.post(PINOT_BROKER_URL, json=query)

# Check the response status
if response.status_code == 200:
    # Convert the response to a Pandas DataFrame
    result = response.json()
    columns = result['resultTable']['dataSchema']['columnNames']
    rows = result['resultTable']['rows']
    df = pd.DataFrame(rows, columns=columns)
    print(df)
else:
    print(f"Error: {response.status_code}, {response.text}")
```

### **Explanation of Code**

1. **Broker URL**: The Pinot Broker is responsible for receiving queries and returning results. We send our SQL query to the broker using the `/query` endpoint.
2. **Query Definition**: The query is defined in JSON format, which includes the SQL statement.
3. **HTTP Request**: We use Python's `requests.post()` method to send the query to Pinot.
4. **Response Handling**: If the response is successful, the JSON data is converted into a Pandas DataFrame for easier analysis and visualization.

### **Step 4: Performing Analytics**

You can further perform various types of analytics using Python and Pinot, such as:

- **Aggregation**: Use SQL queries to aggregate data in Pinot and visualize it in Python using libraries like `matplotlib`.
- **Data Visualization**: You can plot the results using `matplotlib` or `seaborn`.

**Example - Plotting Average Movie Ratings**:
```python
import matplotlib.pyplot as plt

# Assuming df contains the results from Pinot
plt.figure(figsize=(10, 6))
plt.bar(df['title'], df['avgRating'])
plt.xlabel('Movie Title')
plt.ylabel('Average Rating')
plt.title('Average Movie Ratings')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
```

### **Summary**

1. **Setup Apache Pinot**: Set up a local Pinot instance and ingest data.
2. **Integrate with Python**: Use the Pinot REST API to interact with Pinot from Python.
3. **Query and Analyze Data**: Perform SQL queries and visualize the results using Python.
